{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import process_tweet, lookup\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation de fonctions et de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "Le mot 'not' est présent dans la liste des stopwords en anglais.\n"
     ]
    }
   ],
   "source": [
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)\n",
    "print(stopwords.words(\"english\")[:10])\n",
    "\n",
    "# Vérifier si \"not\" est dans la liste des stopwords\n",
    "if \"not\" in stopwords.words(\"english\"):\n",
    "    print(\"Le mot 'not' est présent dans la liste des stopwords en anglais.\")\n",
    "else:\n",
    "    print(\"Le mot 'not' n'est pas présent dans la liste des stopwords en anglais.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traiter les données\n",
    "\n",
    "Pour tout projet d'apprentissage machine, une fois que vous avez des données, la première est la première est l'entrée en cas de traitement en des entrées utiles pour votre modèle.\n",
    "\n",
    "    Supprimer le bruit : Vous voudrez d'abord éliminer le bruit de vos données, c'est-à-dire supprimer les mots qui n'apportent pas un d'informations sur le contenu. Cela inclut tous tous les mots courants que 'I, you, are, is, etc...' qui neait fournir pas suffisamment d'informations sur le sentiment.\n",
    "    Nous renouez les symboles des actions en bourse, les symboles de retweet, les liens hypertexte et les hashtags car ils ne ne pas fournir beaucoup d'informations sur le sentiment.\n",
    "    Vous voudrez aussi supprimer toute la ponctuation d'un tweet. La raison de faire est ce qui nous traiter les mots avec ou sans ponctuation comme le mot, au lieu de traité « heureux », « heureux » ?, « heureux », « heureux », « heureux », et « heureux »., comme des mots différents.\n",
    "    Enfin, vous voudrez utiliser le dents pour ne conserver qu'une variante de mot. En d'autres, nos traitéons \"motivation\", \"motivated\" et \"motivate\" de façon en les avons en pensions dans la main même \"motiv-\".\n",
    "\n",
    "Nous vous avez la fonction  process_tweetqui fait cela pour vous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour m'aider à former mon modèle de Bayes naïf, je devrai calculer un dictionnaire où les clés sont un tuple (mot, étiquette) et les valeurs sont la fréquence correspondante. Note que les étiquettes que j'utiliserai ici sont 1 pour positif et 0 pour négatif.\n",
    "\n",
    "Je mettrai également en œuvre une fonction d'aide à la recherche qui s'inscrira dans le dictionnaire « freqs », un mot et une étiquette (1 ou 0), et retournera le nombre de fois où le mot et le tuple de l'étiquette apparaissent dans la collection de tweets.\n",
    "\n",
    "Par exemple, avec une liste de tweets [\"je suis plutôt excité\", \"je suis plutôt heureux\"] et l'étiquette 1, la fonction retournera un dictionnaire qui contient les paires clé-valeur suivantes :\n",
    "\n",
    "    (\"plutôt\", 1): 2,\n",
    "    (\"heureux\", 1): 1,\n",
    "    (\"excité\", 1): 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "def count_tweets(result, tweets, ys,use_ngrams=False):\n",
    "   \n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet,use_ngrams=False):\n",
    "\n",
    "            pair = (word, y)\n",
    "            \n",
    "           \n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            \n",
    "            else:\n",
    "                result[pair] = 1\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraîner notre modèle à l'aide de Naive Bayes\n",
    "\n",
    "Naive Bayes est un algorithme qui peut être utilisé pour l'analyse de sentiment. Il faut peu de temps pour l'entraînement et a également un temps de prédiction court.\n",
    "\n",
    "Comment entraîner un classificateur Naive Bayes ?\n",
    "\n",
    "    La première étape de l'entraînement d'un classificateur Naive Bayes consiste à identifier le nombre de classes que vous avez.\n",
    "    Vous allez créer une probabilité pour chaque classe.\n",
    "\n",
    "    P(D_{pos}) est la probabilité que le document soit positif.\n",
    "    P(D_{neg}) est la probabilité que le document soit négatif.\n",
    "    Utilisez les formules suivantes et stockez les valeurs dans un dictionnaire :\n",
    "\n",
    "P(D_{pos}) = D_{pos}\\D\n",
    "\n",
    "P(D_{neg}) = D_{neg}\\D\n",
    "\n",
    "Où D est le nombre total de documents, ou de tweets dans ce cas, D_{pos} est le nombre total de tweets positifs et D_{neg} est le nombre total de tweets négatifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilité a priori et Logprobabilité a priori\n",
    "\n",
    "La probabilité a priori représente la probabilité sous-jacente dans la population cible qu'un tweet soit positif par rapport à négatif. En d'autres termes, si nous n'avions aucune information spécifique et choisissions aveuglément un tweet dans l'ensemble de la population, quelle est la probabilité qu'il soit positif par rapport à négatif ? C'est le \"a priori\".\n",
    "\n",
    "Le a priori est le rapport des probabilités P(Dpos)/P(Dneg).\n",
    "Nous pouvons prendre le logarithme du a priori pour le remettre à l'échelle, et nous l'appellerons le logprior\n",
    "\n",
    "logprior=log(P(Dneg​)\\P(Dpos​)​)=log(Dneg\\​Dpos​​).\n",
    "\n",
    "Notez que log(A\\B​) est le même que log⁡(A)−log⁡(B). Ainsi, le logprior peut également être calculé comme la différence entre deux logarithmes :\n",
    "\n",
    "logprior=log(P(Dpos​))−log(P(Dneg​))=log(Dpos​)−log(Dneg​)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilité positive et négative d'un mot\n",
    "\n",
    "Pour calculer la probabilité positive et négative d'un mot spécifique dans le vocabulaire, nous utiliserons les fréquences suivantes :\n",
    "\n",
    "    freqposfreqpos​ et freqnegfreqneg​ sont les fréquences de ce mot spécifique dans la classe positive ou négative. En d'autres termes, la fréquence positive d'un mot est le nombre de fois où le mot est compté avec l'étiquette 1.\n",
    "    NposNpos​ et NnegNneg​ sont le nombre total de mots positifs et négatifs pour tous les documents (pour tous les tweets), respectivement.\n",
    "    V est le nombre de mots uniques dans l'ensemble complet de documents, pour toutes les classes, qu'elles soient positives ou négatives.\n",
    "\n",
    "    Nous utiliserons ces éléments pour calculer la probabilité positive et négative pour un mot spécifique en utilisant la formule suivante :\n",
    "\n",
    "    𝑃(𝑊𝑝𝑜𝑠)=𝑓𝑟𝑒𝑞𝑝𝑜𝑠+1\\𝑁𝑝𝑜𝑠+𝑉\n",
    "    𝑃(𝑊𝑛𝑒𝑔)=𝑓𝑟𝑒𝑞𝑛𝑒𝑔+1\\𝑁𝑛𝑒𝑔+𝑉\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-vraisemblance\n",
    "\n",
    "Pour calculer la log-vraisemblance de ce même mot, nous pouvons mettre en œuvre les équations suivantes :\n",
    "loglikelihood = log P(W_{pos})\\P(W_{neg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "result = {}\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweets(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freqs = count_tweets({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negations(tweet):\n",
    "    negation_words = set(['not', 'no', 'never', 'none', 'neither', 'nor'])\n",
    "\n",
    "    for i in range(len(tweet)):\n",
    "        if tweet[i] in negation_words and i + 1 < len(tweet):\n",
    "            # Prefix the next word with \"not_\" to indicate negation\n",
    "            tweet[i + 1] = 'not_' + tweet[i + 1]\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_naive_bayes(freqs, train_x, train_y,use_ngrams=False):\n",
    "    \n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    \n",
    "\n",
    "  \n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)    \n",
    "\n",
    "    \n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        if pair[1] > 0:\n",
    "            N_pos += freqs[pair]\n",
    "        else:\n",
    "            N_neg += freqs[pair]\n",
    "    \n",
    "    \n",
    "    D = len(train_y)\n",
    "\n",
    "    \n",
    "    D_pos = np.sum(train_y)\n",
    "\n",
    "    \n",
    "    D_neg = D - D_pos\n",
    "\n",
    "    \n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    \n",
    "    \n",
    "    for word in vocab:\n",
    "        if use_ngrams:\n",
    "            ngram_tokens = word.split()\n",
    "            freq_pos = freqs.get((ngram_tokens, 1), 0)\n",
    "            freq_neg = freqs.get((ngram_tokens, 0), 0)\n",
    "        else:\n",
    "            freq_pos = freqs.get((word, 1), 0)\n",
    "            freq_neg = freqs.get((word, 0), 0)\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)  \n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)  \n",
    "\n",
    "        \n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "\n",
    "    return logprior, loglikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "9391\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "𝑝=𝑙𝑜𝑔𝑝𝑟𝑖𝑜𝑟+∑𝑖𝑁(𝑙𝑜𝑔𝑙𝑖𝑘𝑒𝑙𝑖ℎ𝑜𝑜𝑑𝑖)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "   \n",
    "    \n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    \n",
    "    p = 0\n",
    "\n",
    "    \n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "        \n",
    "        if word in loglikelihood:\n",
    "            \n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'not_happi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def handle_negations(tweet):\n",
    "    negation_words = set(['not', 'no', 'never', 'none', 'neither', 'nor'])\n",
    "\n",
    "    for i in range(len(tweet)):\n",
    "        if tweet[i] in negation_words and i + 1 < len(tweet):\n",
    "            # Prefix the next word with \"not_\" to indicate negation\n",
    "            tweet[i + 1] = 'not_' + tweet[i + 1]\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def process_tweet(tweet,use_ngrams=False):\n",
    "    stemmer = PorterStemmer()\n",
    "    # Exclure spécifiquement le mot \"not\" de la liste des stopwords\n",
    "    stopwords_english = set(stopwords.words('english')) - {'not'}\n",
    "\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    if use_ngrams:\n",
    "        tweet_tokens += [' '.join(gram) for gram in list(ngrams(tweet_tokens, 2))]\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and\n",
    "                word not in string.punctuation):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    # Ajouter l'appel à handle_negations ici\n",
    "    tweets_clean = handle_negations(tweets_clean)\n",
    "\n",
    "    return tweets_clean\n",
    "\n",
    "tweet = 'i am not happy'\n",
    "print(process_tweet(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'not_happi']\n",
      "The expected output is 1.5611658326592506\n"
     ]
    }
   ],
   "source": [
    "tweet = 'i am not happy'\n",
    "result = process_tweet(tweet)\n",
    "print(result)\n",
    "\n",
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is -0.16160076508185292\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_tweet = 'He laughed.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
    "    \n",
    "    accuracy = 0\n",
    "\n",
    "    \n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        \n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "           \n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "           \n",
    "            y_hat_i = 0\n",
    "\n",
    "       \n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    \n",
    "    error = np.mean(np.abs(y_hats - test_y))\n",
    "\n",
    "   \n",
    "    accuracy = 1 - error\n",
    "\n",
    "    \n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9950\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 2.14\n",
      "I am bad -> -1.80\n",
      "this movie should have been great. -> 2.25\n",
      "great -> 2.26\n",
      "great great -> 4.52\n",
      "great great great -> 6.78\n",
      "great great great great -> 9.04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "   \n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.316816485972582"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "my_tweet = 'you are bad :('\n",
    "naive_bayes_predict(my_tweet, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certains mots ont plus de occurrences positives que d'autres et peuvent être considérés comme \"plus positifs\". De même, certains mots peuvent être considérés comme plus négatifs que d'autres.\n",
    "Une façon pour nous de définir le niveau de positivité ou de négativité, sans calculer la log-vraisemblance, consiste à comparer la fréquence positive à la fréquence négative du mot.\n",
    "    Notez que nous pouvons également utiliser les calculs de log-vraisemblance pour comparer la positivité ou la négativité relative des mots.\n",
    "Nous pouvons calculer le rapport des fréquences positives aux fréquences négatives d'un mot.\n",
    "Une fois que nous sommes capables de calculer ces ratios, nous pouvons également filtrer un sous-ensemble de mots ayant un rapport minimum de positivité / négativité ou plus.\n",
    "De même, nous pouvons également filtrer un sous-ensemble de mots ayant un rapport maximum de positivité / négativité ou moins (des mots qui sont au moins aussi négatifs, voire plus négatifs, qu'un seuil donné).\n",
    "\n",
    "𝑟𝑎𝑡𝑖𝑜=pos_words+1\\neg_words+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ratio(freqs, word):\n",
    "    \n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "    \n",
    "    \n",
    "    pos_words = lookup(freqs, word, 1)\n",
    "    \n",
    "    \n",
    "    neg_words = lookup(freqs, word, 0)\n",
    "    \n",
    "    \n",
    "    pos_neg_ratio['ratio'] = (pos_words + 1) / (neg_words + 1)\n",
    "    pos_neg_ratio['positive'] = pos_words\n",
    "    pos_neg_ratio['negative'] = neg_words\n",
    "   \n",
    "    \n",
    "    return pos_neg_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 161, 'negative': 18, 'ratio': 8.526315789473685}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ratio(freqs, 'happi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_words_by_threshold(freqs, label, threshold, get_ratio=get_ratio):\n",
    "    \n",
    "    word_list = {}\n",
    "\n",
    "    \n",
    "    for word, _ in freqs.keys():\n",
    "        \n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "        \n",
    "        \n",
    "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
    "            \n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "       \n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "            \n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        \n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{':(': {'positive': 1, 'negative': 3652, 'ratio': 0.0005474952094169176},\n",
       " ':-(': {'positive': 0, 'negative': 384, 'ratio': 0.0025974025974025974},\n",
       " 'zayniscomingbackonjuli': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " '26': {'positive': 0, 'negative': 20, 'ratio': 0.047619047619047616},\n",
       " '>:(': {'positive': 0, 'negative': 43, 'ratio': 0.022727272727272728},\n",
       " 'lost': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " 'not_:(': {'positive': 0, 'negative': 23, 'ratio': 0.041666666666666664},\n",
       " '♛': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " '》': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " 'beli̇ev': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'wi̇ll': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'justi̇n': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ｓｅｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ｍｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_words_by_threshold(freqs, label=0, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'followfriday': {'positive': 23, 'negative': 0, 'ratio': 24.0},\n",
       " 'commun': {'positive': 27, 'negative': 1, 'ratio': 14.0},\n",
       " ':)': {'positive': 2954, 'negative': 2, 'ratio': 985.0},\n",
       " 'flipkartfashionfriday': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " 'friday': {'positive': 91, 'negative': 8, 'ratio': 10.222222222222221},\n",
       " ':d': {'positive': 523, 'negative': 0, 'ratio': 524.0},\n",
       " ':p': {'positive': 105, 'negative': 0, 'ratio': 106.0},\n",
       " 'influenc': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " ':-)': {'positive': 551, 'negative': 0, 'ratio': 552.0},\n",
       " \"here'\": {'positive': 20, 'negative': 0, 'ratio': 21.0},\n",
       " 'youth': {'positive': 14, 'negative': 0, 'ratio': 15.0},\n",
       " 'bam': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'warsaw': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'shout': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " ';)': {'positive': 22, 'negative': 0, 'ratio': 23.0},\n",
       " 'stat': {'positive': 51, 'negative': 0, 'ratio': 52.0},\n",
       " 'arriv': {'positive': 57, 'negative': 4, 'ratio': 11.6},\n",
       " 'glad': {'positive': 41, 'negative': 2, 'ratio': 14.0},\n",
       " 'blog': {'positive': 27, 'negative': 0, 'ratio': 28.0},\n",
       " 'fav': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " 'fantast': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
       " 'fback': {'positive': 26, 'negative': 0, 'ratio': 27.0},\n",
       " 'pleasur': {'positive': 10, 'negative': 0, 'ratio': 11.0},\n",
       " '←': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
       " 'aqui': {'positive': 9, 'negative': 0, 'ratio': 10.0}}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_words_by_threshold(freqs, label=1, threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Predicted Tweet\n",
      "1\t0.00\tb'truli later move know queen bee upward bound movingonup'\n",
      "1\t0.00\tb'new report talk burn calori cold work harder warm feel better weather :p'\n",
      "1\t0.00\tb'harri niall 94 harri born ik stupid wanna chang :d'\n",
      "1\t0.00\tb'park get sunlight'\n",
      "1\t0.00\tb'uff itna miss karhi thi ap :p'\n",
      "0\t1.00\tb'hello info possibl interest jonatha close join beti :( great'\n",
      "0\t1.00\tb'u prob fun david'\n",
      "0\t1.00\tb'pat jay'\n",
      "0\t1.00\tb'whatev stil l young >:-('\n",
      "0\t1.00\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2035581379053828\n"
     ]
    }
   ],
   "source": [
    "#!pip install googletrans==4.0.0-rc1\n",
    "from googletrans import Translator\n",
    "import pandas as pd\n",
    "file_path = \"C:\\\\Users\\\\hp\\Desktop\\\\projet_machine_learning\\\\FILE_NAME.xls\"\n",
    "\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "colonne_ressenti = df.iloc[:, 6]  \n",
    "translator = Translator()\n",
    "\n",
    "# Traduire chaque phrase de la sixième colonne et stocker les résultats dans une nouvelle colonne\n",
    "#df['Translated_Column'] = colonne_ressenti.apply(lambda phrase: translator.translate(phrase, dest='en').text)\n",
    "#df['Translated_Column1'] = df['Translated_Column'].map(str)\n",
    "\n",
    "\n",
    "#unique_values = df['Translated_Column'][0]\n",
    "#print(unique_values)\n",
    "\n",
    "#unique_values='i am not happy  because i did  go   '\n",
    "#unique_values='i am  happy  because i did not go   '\n",
    "#unique_values='this is not good because your attitude is not even close to being nice'\n",
    "\n",
    "#unique_values='Although I enjoy learning about machine learning and deep learning theories, facing coding errors has made me detest this model entirely'\n",
    "#unique_values='I love this product! This is terrible'\n",
    "unique_values='Oh, great, another alarm clock malfunction.'\n",
    "p = naive_bayes_predict(unique_values, logprior, loglikelihood)\n",
    "print(p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.757\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.76       988\n",
      "           1       0.78      0.73      0.75      1012\n",
      "\n",
      "    accuracy                           0.76      2000\n",
      "   macro avg       0.76      0.76      0.76      2000\n",
      "weighted avg       0.76      0.76      0.76      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "# Charger les données de tweets positifs et négatifs\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# Créer les étiquettes correspondantes\n",
    "positive_labels = [1] * len(all_positive_tweets)\n",
    "negative_labels = [0] * len(all_negative_tweets)\n",
    "\n",
    "# Concaténer les données et les étiquettes\n",
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "labels = positive_labels + negative_labels\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorisation des données textuelles\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialiser et entraîner le modèle SVM\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "predictions = svm_model.predict(X_test_vectorized)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch==1.10.0\n",
    "\n",
    "! pip install transformers==4.12.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Charger le modèle pré-entraîné BERT pour la classification de séquence\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Phrase à évaluer\n",
    "phrase = \"Oh, great, another alarm clock malfunction.\"\n",
    "\n",
    "# Tokeniser la phrase\n",
    "inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "\n",
    "# Faire la prédiction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtenir les probabilités de chaque classe\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Afficher les probabilités\n",
    "print(\"Probabilités de chaque classe:\", probs)\n",
    "\n",
    "# Classe prédite\n",
    "predicted_class = torch.argmax(probs).item()\n",
    "print(\"Classe prédite:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
