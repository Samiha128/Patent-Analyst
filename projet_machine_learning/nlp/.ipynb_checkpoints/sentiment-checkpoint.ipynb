{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import process_tweet, lookup\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation de fonctions et de donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "Le mot 'not' est prÃ©sent dans la liste des stopwords en anglais.\n"
     ]
    }
   ],
   "source": [
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)\n",
    "print(stopwords.words(\"english\")[:10])\n",
    "\n",
    "# VÃ©rifier si \"not\" est dans la liste des stopwords\n",
    "if \"not\" in stopwords.words(\"english\"):\n",
    "    print(\"Le mot 'not' est prÃ©sent dans la liste des stopwords en anglais.\")\n",
    "else:\n",
    "    print(\"Le mot 'not' n'est pas prÃ©sent dans la liste des stopwords en anglais.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traiter les donnÃ©es\n",
    "\n",
    "Pour tout projet d'apprentissage machine, une fois que vous avez des donnÃ©es, la premiÃ¨re est la premiÃ¨re est l'entrÃ©e en cas de traitement en des entrÃ©es utiles pour votre modÃ¨le.\n",
    "\n",
    "    Supprimer le bruit : Vous voudrez d'abord Ã©liminer le bruit de vos donnÃ©es, c'est-Ã -dire supprimer les mots qui n'apportent pas un d'informations sur le contenu. Cela inclut tous tous les mots courants que 'I, you, are, is, etc...' qui neait fournir pas suffisamment d'informations sur le sentiment.\n",
    "    Nous renouez les symboles des actions en bourse, les symboles de retweet, les liens hypertexte et les hashtags car ils ne ne pas fournir beaucoup d'informations sur le sentiment.\n",
    "    Vous voudrez aussi supprimer toute la ponctuation d'un tweet. La raison de faire est ce qui nous traiter les mots avec ou sans ponctuation comme le mot, au lieu de traitÃ© Â« heureux Â», Â« heureux Â» ?, Â« heureux Â», Â« heureux Â», Â« heureux Â», et Â« heureux Â»., comme des mots diffÃ©rents.\n",
    "    Enfin, vous voudrez utiliser le dents pour ne conserver qu'une variante de mot. En d'autres, nos traitÃ©ons \"motivation\", \"motivated\" et \"motivate\" de faÃ§on en les avons en pensions dans la main mÃªme \"motiv-\".\n",
    "\n",
    "Nous vous avez la fonction  process_tweetqui fait cela pour vous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour m'aider Ã  former mon modÃ¨le de Bayes naÃ¯f, je devrai calculer un dictionnaire oÃ¹ les clÃ©s sont un tuple (mot, Ã©tiquette) et les valeurs sont la frÃ©quence correspondante. Note que les Ã©tiquettes que j'utiliserai ici sont 1 pour positif et 0 pour nÃ©gatif.\n",
    "\n",
    "Je mettrai Ã©galement en Å“uvre une fonction d'aide Ã  la recherche qui s'inscrira dans le dictionnaire Â« freqs Â», un mot et une Ã©tiquette (1 ou 0), et retournera le nombre de fois oÃ¹ le mot et le tuple de l'Ã©tiquette apparaissent dans la collection de tweets.\n",
    "\n",
    "Par exemple, avec une liste de tweets [\"je suis plutÃ´t excitÃ©\", \"je suis plutÃ´t heureux\"] et l'Ã©tiquette 1, la fonction retournera un dictionnaire qui contient les paires clÃ©-valeur suivantes :\n",
    "\n",
    "    (\"plutÃ´t\", 1): 2,\n",
    "    (\"heureux\", 1): 1,\n",
    "    (\"excitÃ©\", 1): 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "def count_tweets(result, tweets, ys,use_ngrams=False):\n",
    "   \n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet,use_ngrams=False):\n",
    "\n",
    "            pair = (word, y)\n",
    "            \n",
    "           \n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            \n",
    "            else:\n",
    "                result[pair] = 1\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EntraÃ®ner notre modÃ¨le Ã  l'aide de Naive Bayes\n",
    "\n",
    "Naive Bayes est un algorithme qui peut Ãªtre utilisÃ© pour l'analyse de sentiment. Il faut peu de temps pour l'entraÃ®nement et a Ã©galement un temps de prÃ©diction court.\n",
    "\n",
    "Comment entraÃ®ner un classificateur Naive Bayes ?\n",
    "\n",
    "    La premiÃ¨re Ã©tape de l'entraÃ®nement d'un classificateur Naive Bayes consiste Ã  identifier le nombre de classes que vous avez.\n",
    "    Vous allez crÃ©er une probabilitÃ© pour chaque classe.\n",
    "\n",
    "    P(D_{pos}) est la probabilitÃ© que le document soit positif.\n",
    "    P(D_{neg}) est la probabilitÃ© que le document soit nÃ©gatif.\n",
    "    Utilisez les formules suivantes et stockez les valeurs dans un dictionnaire :\n",
    "\n",
    "P(D_{pos}) = D_{pos}\\D\n",
    "\n",
    "P(D_{neg}) = D_{neg}\\D\n",
    "\n",
    "OÃ¹ D est le nombre total de documents, ou de tweets dans ce cas, D_{pos} est le nombre total de tweets positifs et D_{neg} est le nombre total de tweets nÃ©gatifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProbabilitÃ© a priori et LogprobabilitÃ© a priori\n",
    "\n",
    "La probabilitÃ© a priori reprÃ©sente la probabilitÃ© sous-jacente dans la population cible qu'un tweet soit positif par rapport Ã  nÃ©gatif. En d'autres termes, si nous n'avions aucune information spÃ©cifique et choisissions aveuglÃ©ment un tweet dans l'ensemble de la population, quelle est la probabilitÃ© qu'il soit positif par rapport Ã  nÃ©gatif ? C'est le \"a priori\".\n",
    "\n",
    "Le a priori est le rapport des probabilitÃ©s P(Dpos)/P(Dneg).\n",
    "Nous pouvons prendre le logarithme du a priori pour le remettre Ã  l'Ã©chelle, et nous l'appellerons le logprior\n",
    "\n",
    "logprior=log(P(Dnegâ€‹)\\P(Dposâ€‹)â€‹)=log(Dneg\\â€‹Dposâ€‹â€‹).\n",
    "\n",
    "Notez que log(A\\Bâ€‹) est le mÃªme que logâ¡(A)âˆ’logâ¡(B). Ainsi, le logprior peut Ã©galement Ãªtre calculÃ© comme la diffÃ©rence entre deux logarithmes :\n",
    "\n",
    "logprior=log(P(Dposâ€‹))âˆ’log(P(Dnegâ€‹))=log(Dposâ€‹)âˆ’log(Dnegâ€‹)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProbabilitÃ© positive et nÃ©gative d'un mot\n",
    "\n",
    "Pour calculer la probabilitÃ© positive et nÃ©gative d'un mot spÃ©cifique dans le vocabulaire, nous utiliserons les frÃ©quences suivantes :\n",
    "\n",
    "    freqposfreqposâ€‹ et freqnegfreqnegâ€‹ sont les frÃ©quences de ce mot spÃ©cifique dans la classe positive ou nÃ©gative. En d'autres termes, la frÃ©quence positive d'un mot est le nombre de fois oÃ¹ le mot est comptÃ© avec l'Ã©tiquette 1.\n",
    "    NposNposâ€‹ et NnegNnegâ€‹ sont le nombre total de mots positifs et nÃ©gatifs pour tous les documents (pour tous les tweets), respectivement.\n",
    "    V est le nombre de mots uniques dans l'ensemble complet de documents, pour toutes les classes, qu'elles soient positives ou nÃ©gatives.\n",
    "\n",
    "    Nous utiliserons ces Ã©lÃ©ments pour calculer la probabilitÃ© positive et nÃ©gative pour un mot spÃ©cifique en utilisant la formule suivante :\n",
    "\n",
    "    ğ‘ƒ(ğ‘Šğ‘ğ‘œğ‘ )=ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘ğ‘œğ‘ +1\\ğ‘ğ‘ğ‘œğ‘ +ğ‘‰\n",
    "    ğ‘ƒ(ğ‘Šğ‘›ğ‘’ğ‘”)=ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘›ğ‘’ğ‘”+1\\ğ‘ğ‘›ğ‘’ğ‘”+ğ‘‰\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-vraisemblance\n",
    "\n",
    "Pour calculer la log-vraisemblance de ce mÃªme mot, nous pouvons mettre en Å“uvre les Ã©quations suivantes :\n",
    "loglikelihood = log P(W_{pos})\\P(W_{neg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "result = {}\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweets(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freqs = count_tweets({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negations(tweet):\n",
    "    negation_words = set(['not', 'no', 'never', 'none', 'neither', 'nor'])\n",
    "\n",
    "    for i in range(len(tweet)):\n",
    "        if tweet[i] in negation_words and i + 1 < len(tweet):\n",
    "            # Prefix the next word with \"not_\" to indicate negation\n",
    "            tweet[i + 1] = 'not_' + tweet[i + 1]\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_naive_bayes(freqs, train_x, train_y,use_ngrams=False):\n",
    "    \n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    \n",
    "\n",
    "  \n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)    \n",
    "\n",
    "    \n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        if pair[1] > 0:\n",
    "            N_pos += freqs[pair]\n",
    "        else:\n",
    "            N_neg += freqs[pair]\n",
    "    \n",
    "    \n",
    "    D = len(train_y)\n",
    "\n",
    "    \n",
    "    D_pos = np.sum(train_y)\n",
    "\n",
    "    \n",
    "    D_neg = D - D_pos\n",
    "\n",
    "    \n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    \n",
    "    \n",
    "    for word in vocab:\n",
    "        if use_ngrams:\n",
    "            ngram_tokens = word.split()\n",
    "            freq_pos = freqs.get((ngram_tokens, 1), 0)\n",
    "            freq_neg = freqs.get((ngram_tokens, 0), 0)\n",
    "        else:\n",
    "            freq_pos = freqs.get((word, 1), 0)\n",
    "            freq_neg = freqs.get((word, 0), 0)\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)  \n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)  \n",
    "\n",
    "        \n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "\n",
    "    return logprior, loglikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "9391\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğ‘=ğ‘™ğ‘œğ‘”ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ+âˆ‘ğ‘–ğ‘(ğ‘™ğ‘œğ‘”ğ‘™ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ğ‘œğ‘œğ‘‘ğ‘–)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "   \n",
    "    \n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    \n",
    "    p = 0\n",
    "\n",
    "    \n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "        \n",
    "        if word in loglikelihood:\n",
    "            \n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'not_happi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def handle_negations(tweet):\n",
    "    negation_words = set(['not', 'no', 'never', 'none', 'neither', 'nor'])\n",
    "\n",
    "    for i in range(len(tweet)):\n",
    "        if tweet[i] in negation_words and i + 1 < len(tweet):\n",
    "            # Prefix the next word with \"not_\" to indicate negation\n",
    "            tweet[i + 1] = 'not_' + tweet[i + 1]\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def process_tweet(tweet,use_ngrams=False):\n",
    "    stemmer = PorterStemmer()\n",
    "    # Exclure spÃ©cifiquement le mot \"not\" de la liste des stopwords\n",
    "    stopwords_english = set(stopwords.words('english')) - {'not'}\n",
    "\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    if use_ngrams:\n",
    "        tweet_tokens += [' '.join(gram) for gram in list(ngrams(tweet_tokens, 2))]\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and\n",
    "                word not in string.punctuation):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    # Ajouter l'appel Ã  handle_negations ici\n",
    "    tweets_clean = handle_negations(tweets_clean)\n",
    "\n",
    "    return tweets_clean\n",
    "\n",
    "tweet = 'i am not happy'\n",
    "print(process_tweet(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'not_happi']\n",
      "The expected output is 1.5611658326592506\n"
     ]
    }
   ],
   "source": [
    "tweet = 'i am not happy'\n",
    "result = process_tweet(tweet)\n",
    "print(result)\n",
    "\n",
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is -0.16160076508185292\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_tweet = 'He laughed.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
    "    \n",
    "    accuracy = 0\n",
    "\n",
    "    \n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        \n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "           \n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "           \n",
    "            y_hat_i = 0\n",
    "\n",
    "       \n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    \n",
    "    error = np.mean(np.abs(y_hats - test_y))\n",
    "\n",
    "   \n",
    "    accuracy = 1 - error\n",
    "\n",
    "    \n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9950\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 2.14\n",
      "I am bad -> -1.80\n",
      "this movie should have been great. -> 2.25\n",
      "great -> 2.26\n",
      "great great -> 4.52\n",
      "great great great -> 6.78\n",
      "great great great great -> 9.04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "   \n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.316816485972582"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "my_tweet = 'you are bad :('\n",
    "naive_bayes_predict(my_tweet, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certains mots ont plus de occurrences positives que d'autres et peuvent Ãªtre considÃ©rÃ©s comme \"plus positifs\". De mÃªme, certains mots peuvent Ãªtre considÃ©rÃ©s comme plus nÃ©gatifs que d'autres.\n",
    "Une faÃ§on pour nous de dÃ©finir le niveau de positivitÃ© ou de nÃ©gativitÃ©, sans calculer la log-vraisemblance, consiste Ã  comparer la frÃ©quence positive Ã  la frÃ©quence nÃ©gative du mot.\n",
    "    Notez que nous pouvons Ã©galement utiliser les calculs de log-vraisemblance pour comparer la positivitÃ© ou la nÃ©gativitÃ© relative des mots.\n",
    "Nous pouvons calculer le rapport des frÃ©quences positives aux frÃ©quences nÃ©gatives d'un mot.\n",
    "Une fois que nous sommes capables de calculer ces ratios, nous pouvons Ã©galement filtrer un sous-ensemble de mots ayant un rapport minimum de positivitÃ© / nÃ©gativitÃ© ou plus.\n",
    "De mÃªme, nous pouvons Ã©galement filtrer un sous-ensemble de mots ayant un rapport maximum de positivitÃ© / nÃ©gativitÃ© ou moins (des mots qui sont au moins aussi nÃ©gatifs, voire plus nÃ©gatifs, qu'un seuil donnÃ©).\n",
    "\n",
    "ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œ=pos_words+1\\neg_words+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ratio(freqs, word):\n",
    "    \n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "    \n",
    "    \n",
    "    pos_words = lookup(freqs, word, 1)\n",
    "    \n",
    "    \n",
    "    neg_words = lookup(freqs, word, 0)\n",
    "    \n",
    "    \n",
    "    pos_neg_ratio['ratio'] = (pos_words + 1) / (neg_words + 1)\n",
    "    pos_neg_ratio['positive'] = pos_words\n",
    "    pos_neg_ratio['negative'] = neg_words\n",
    "   \n",
    "    \n",
    "    return pos_neg_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 161, 'negative': 18, 'ratio': 8.526315789473685}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ratio(freqs, 'happi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_words_by_threshold(freqs, label, threshold, get_ratio=get_ratio):\n",
    "    \n",
    "    word_list = {}\n",
    "\n",
    "    \n",
    "    for word, _ in freqs.keys():\n",
    "        \n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "        \n",
    "        \n",
    "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
    "            \n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "       \n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "            \n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        \n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{':(': {'positive': 1, 'negative': 3652, 'ratio': 0.0005474952094169176},\n",
       " ':-(': {'positive': 0, 'negative': 384, 'ratio': 0.0025974025974025974},\n",
       " 'zayniscomingbackonjuli': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " '26': {'positive': 0, 'negative': 20, 'ratio': 0.047619047619047616},\n",
       " '>:(': {'positive': 0, 'negative': 43, 'ratio': 0.022727272727272728},\n",
       " 'lost': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " 'not_:(': {'positive': 0, 'negative': 23, 'ratio': 0.041666666666666664},\n",
       " 'â™›': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " 'ã€‹': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " 'beliÌ‡ev': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'wiÌ‡ll': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'justiÌ‡n': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ï½“ï½…ï½…': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ï½ï½…': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_words_by_threshold(freqs, label=0, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'followfriday': {'positive': 23, 'negative': 0, 'ratio': 24.0},\n",
       " 'commun': {'positive': 27, 'negative': 1, 'ratio': 14.0},\n",
       " ':)': {'positive': 2954, 'negative': 2, 'ratio': 985.0},\n",
       " 'flipkartfashionfriday': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " 'friday': {'positive': 91, 'negative': 8, 'ratio': 10.222222222222221},\n",
       " ':d': {'positive': 523, 'negative': 0, 'ratio': 524.0},\n",
       " ':p': {'positive': 105, 'negative': 0, 'ratio': 106.0},\n",
       " 'influenc': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " ':-)': {'positive': 551, 'negative': 0, 'ratio': 552.0},\n",
       " \"here'\": {'positive': 20, 'negative': 0, 'ratio': 21.0},\n",
       " 'youth': {'positive': 14, 'negative': 0, 'ratio': 15.0},\n",
       " 'bam': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'warsaw': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'shout': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " ';)': {'positive': 22, 'negative': 0, 'ratio': 23.0},\n",
       " 'stat': {'positive': 51, 'negative': 0, 'ratio': 52.0},\n",
       " 'arriv': {'positive': 57, 'negative': 4, 'ratio': 11.6},\n",
       " 'glad': {'positive': 41, 'negative': 2, 'ratio': 14.0},\n",
       " 'blog': {'positive': 27, 'negative': 0, 'ratio': 28.0},\n",
       " 'fav': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " 'fantast': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
       " 'fback': {'positive': 26, 'negative': 0, 'ratio': 27.0},\n",
       " 'pleasur': {'positive': 10, 'negative': 0, 'ratio': 11.0},\n",
       " 'â†': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
       " 'aqui': {'positive': 9, 'negative': 0, 'ratio': 10.0}}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_words_by_threshold(freqs, label=1, threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Predicted Tweet\n",
      "1\t0.00\tb'truli later move know queen bee upward bound movingonup'\n",
      "1\t0.00\tb'new report talk burn calori cold work harder warm feel better weather :p'\n",
      "1\t0.00\tb'harri niall 94 harri born ik stupid wanna chang :d'\n",
      "1\t0.00\tb'park get sunlight'\n",
      "1\t0.00\tb'uff itna miss karhi thi ap :p'\n",
      "0\t1.00\tb'hello info possibl interest jonatha close join beti :( great'\n",
      "0\t1.00\tb'u prob fun david'\n",
      "0\t1.00\tb'pat jay'\n",
      "0\t1.00\tb'whatev stil l young >:-('\n",
      "0\t1.00\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2035581379053828\n"
     ]
    }
   ],
   "source": [
    "#!pip install googletrans==4.0.0-rc1\n",
    "from googletrans import Translator\n",
    "import pandas as pd\n",
    "file_path = \"C:\\\\Users\\\\hp\\Desktop\\\\projet_machine_learning\\\\FILE_NAME.xls\"\n",
    "\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "colonne_ressenti = df.iloc[:, 6]  \n",
    "translator = Translator()\n",
    "\n",
    "# Traduire chaque phrase de la sixiÃ¨me colonne et stocker les rÃ©sultats dans une nouvelle colonne\n",
    "#df['Translated_Column'] = colonne_ressenti.apply(lambda phrase: translator.translate(phrase, dest='en').text)\n",
    "#df['Translated_Column1'] = df['Translated_Column'].map(str)\n",
    "\n",
    "\n",
    "#unique_values = df['Translated_Column'][0]\n",
    "#print(unique_values)\n",
    "\n",
    "#unique_values='i am not happy  because i did  go   '\n",
    "#unique_values='i am  happy  because i did not go   '\n",
    "#unique_values='this is not good because your attitude is not even close to being nice'\n",
    "\n",
    "#unique_values='Although I enjoy learning about machine learning and deep learning theories, facing coding errors has made me detest this model entirely'\n",
    "#unique_values='I love this product! This is terrible'\n",
    "unique_values='Oh, great, another alarm clock malfunction.'\n",
    "p = naive_bayes_predict(unique_values, logprior, loglikelihood)\n",
    "print(p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.757\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.76       988\n",
      "           1       0.78      0.73      0.75      1012\n",
      "\n",
      "    accuracy                           0.76      2000\n",
      "   macro avg       0.76      0.76      0.76      2000\n",
      "weighted avg       0.76      0.76      0.76      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "# Charger les donnÃ©es de tweets positifs et nÃ©gatifs\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# CrÃ©er les Ã©tiquettes correspondantes\n",
    "positive_labels = [1] * len(all_positive_tweets)\n",
    "negative_labels = [0] * len(all_negative_tweets)\n",
    "\n",
    "# ConcatÃ©ner les donnÃ©es et les Ã©tiquettes\n",
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "labels = positive_labels + negative_labels\n",
    "\n",
    "# Diviser les donnÃ©es en ensembles d'entraÃ®nement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorisation des donnÃ©es textuelles\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialiser et entraÃ®ner le modÃ¨le SVM\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Faire des prÃ©dictions sur l'ensemble de test\n",
    "predictions = svm_model.predict(X_test_vectorized)\n",
    "\n",
    "# Ã‰valuer les performances du modÃ¨le\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch==1.10.0\n",
    "\n",
    "! pip install transformers==4.12.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Charger le modÃ¨le prÃ©-entraÃ®nÃ© BERT pour la classification de sÃ©quence\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Phrase Ã  Ã©valuer\n",
    "phrase = \"Oh, great, another alarm clock malfunction.\"\n",
    "\n",
    "# Tokeniser la phrase\n",
    "inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "\n",
    "# Faire la prÃ©diction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtenir les probabilitÃ©s de chaque classe\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Afficher les probabilitÃ©s\n",
    "print(\"ProbabilitÃ©s de chaque classe:\", probs)\n",
    "\n",
    "# Classe prÃ©dite\n",
    "predicted_class = torch.argmax(probs).item()\n",
    "print(\"Classe prÃ©dite:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
